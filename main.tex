%%% Example for master's thesis (in English)
% \documentclass[english]{ampmt} % pdflatex
\documentclass[dvipdfmx,english]{ampmt} % dvipdfmx

%----for warning
% \DeclareFontShape{JT2}{mc}{m}{it}{<->ssub*mc/m/n}{}
% \DeclareFontShape{JT2}{mc}{m}{sl}{<->ssub*mc/m/n}{}
% \DeclareFontShape{JT2}{mc}{m}{sc}{<->ssub*mc/m/n}{}
% \DeclareFontShape{JT2}{gt}{m}{it}{<->ssub*gt/m/n}{}
% \DeclareFontShape{JT2}{gt}{m}{sl}{<->ssub*gt/m/n}{}
% \DeclareFontShape{JT2}{mc}{bx}{it}{<->ssub*gt/m/n}{}
% \DeclareFontShape{JT2}{mc}{bx}{sl}{<->ssub*gt/m/n}{}
% %
% \DeclareFontShape{JY2}{mc}{m}{it}{<->ssub*mc/m/n}{}
% \DeclareFontShape{JY2}{mc}{m}{sl}{<->ssub*mc/m/n}{}
% \DeclareFontShape{JY2}{mc}{m}{sc}{<->ssub*mc/m/n}{}
% \DeclareFontShape{JY2}{gt}{m}{it}{<->ssub*gt/m/n}{}
% \DeclareFontShape{JY2}{gt}{m}{sl}{<->ssub*gt/m/n}{}
% \DeclareFontShape{JY2}{mc}{bx}{it}{<->ssub*gt/m/n}{}
% \DeclareFontShape{JY2}{mc}{bx}{sl}{<->ssub*gt/m/n}{}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
%% from here-------------------------
\makeatletter
\DeclareRobustCommand{\qed}{%
  \ifmmode % if math mode, assume display: omit penalty etc.
  \else \leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
  \fi
  \quad\hbox{\qedsymbol}}
\newcommand{\openbox}{\leavevmode
  \hbox to.77778em{%
  \hfil\vrule
  \vbox to.675em{\hrule width.6em\vfil\hrule}%
  \vrule\hfil}}
\newcommand{\qedsymbol}{\openbox}
\newenvironment{proof}[1][\proofname]{\par
  \normalfont
  \topsep6\p@\@plus6\p@ \trivlist
  \item[\hskip\labelsep\itshape
    #1.]\ignorespaces
}{%
  \qed\endtrivlist
}
\newcommand{\proofname}{Proof}
\makeatother
%% upto here----------------------------
%--- Title ----------------------------------------------------------------------
\title[Study on a further improvement of \\ Maurer's universal statistical test]
      {Study on a further improvement of \\  Maurer's universal statistical test}
      % [title for spine (option)]{title}
%--- Supervisors ----------------------------------------------------------------
\supervisors{Ken UMENO}{Professor}             % First supervisor  {name}{title}
            {Atsushi IWASAKI}{Assistant Professor} % Second supervisor {name}{title}
            {}{}                               % Third supervisor  {name}{title}
%--- Author ---------------------------------------------------------------------
\author{Yasunari HIKIMA}
%-- Submission date -------------------------------------------------------------
\submissiondate{2020}{February}   % {year}{month}
%-- Width of a spine ------------------------------------------------------------
\setlength{\wdspine}{15mm}
%-- Number of output spines -----------------------------------------------------
\def\numberofspines{1}
%-- Abstract --------------------------------------------------------------------
\abstract{%
Maurer's universal statistical test is a hypothesis test for evaluating the randomness of a binary sequence and it is included in NIST SP 800-22 which is one of the most famous test suites. 
%
The test statistic of Maurer's test relates to the entropy of a tested sequence and hence the test can detect various defects of the sequence about randomness.
It has been reported that flipping a part of bits in a sequence makes Maurer's test more sensitive.
%
The test with flipping is called highly sensitive universal statistical test. To perform the highly sensitive test, the variance for the reference distribution is necessary, however, the theoretical value has not been derived.
%
In this thesis, we theoretically derive the variance for the reference distribution of the highly sensitive test and investigate the validity for testing randomness.
}
%-- Packages and definitions of your own macros ---------------------------------
\usepackage{amsmath,amssymb}
% \usepackage{amsthm}
\usepackage{newtxtext,newtxmath} % Times font
\newcommand{\rme}{\mathrm{e}}
\usepackage{here}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{enumerate}
\usetikzlibrary{patterns}
%-- Control of output -----------------------------------------------------------
\begin{document}
\ifoutputbody
%-- Inside cover, abstract and table of contents ---------------------------------
\makeinsidecover                % Inside cover
\makeabstract                   % Abstract
\maketoc                        % Table of contents
\setcounter{page}{1}
%-- Body -------------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Random sequence}%乱数列 %definition/application/generation
%乱数列は直感的には何のパターンや規則も持たない数字の列であると考えることができるが，その定義を明確に述べることは極めて難しい．なぜなら，ある系列が他のある系列よりもよりランダムであると述べる
\input{intro}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Tests for randomness}\label{subsec:1-2}%乱数検定
\input{tests_for_randomness}
%-----------------------------------------------------------------------------------------------%
\subsection{Outline}%概要
\input{outline}
\newpage
\section{Universal statistical test}\label{sec:universal}
%The purpose of the test is to detect whether or not the sequence can be significantly compressed without loss of information. A compressible sequence is considered to be non­ random.
% In this section, we first state the universal statistical test proposed by Maurer \cite{maurer1992universal} and by Coron \cite{coron1999security}. In the next place, we show the Highly Sensitive Universal Statistical Test proposed by Yamamoto and Liu \cite{yamamoto2016highly}.
In this section, we introduce ``Maurer's universal statistical test'' \cite{maurer1992universal}, ``Coron's universal statistical test' \cite{coron1999security} and ``highly sensitive universal statistical test'' \cite{yamamoto2016highly}.
%
\input{section_maurer.tex}
%-----------------------------------------------------------------------------------------------%
\newpage
\section{Distribution}\label{sec:distribution}
In this section, we consider the distribution of $A_n(\hat{x}^n)$, where $\hat{x}^n$ is an $n$-bit random variable. For each $i$, we have
\begin{align}
  \mathrm{Pr}[(\hat{x}^n)_i = 1] = \hat{q},
\end{align}
where $(\hat{x}^n)_i$ is the $i$-th bit of $\hat{x}^n$. Each bit $(\hat{x}^n)_i$ is independent from other bits.
For simplicity, we write $A_n(\hat{x}^n)$ as $A_n$ unless specified.
In the following, we consider an assumption $Q\to\infty$, and due to the situation, the index of $A_n$ should be replaced as illustrated in Figure \ref{fig:replace}. Then, a sequence of $\{A_k\}_{k=1}^{K}$ follows a stationary ergodic process, that is, the joint distribution of $\{A_k\}_{k=n}^{n+m}$ only depends on $m$.  
In this section, we derive a marginal distribution of $A_n$ and a joint distribution of $(A_n,A_{n+k})$ necessary for calculating the variance of the reference distribution of the highly sensitive test.
%-----------------------------------------------------------------------------------------------%
\begin{figure}[b]
\centering
\begin{tikzpicture}
\draw (-1.0,0) rectangle (0,1);
\draw node at (-0.5,0.2) [above] {$A_1$};
\draw (0,0) rectangle (1,1);
\draw node at (0.5,0.2) [above] {$A_2$};
\draw (1,0) rectangle (2,1);
\draw node at (1.5,0.2) [above] {$A_3$};
\draw (2,0) rectangle (3,1);
\draw node at (2.5,0.2) [above] {$A_4$};
\draw (3,0)--(3.5,0);
\draw (3,1)--(3.5,1);
\draw[loosely dotted, very thick] (3.75,0.5)--(4.25,0.5);
\draw (4.5,0)--(5.0,0);
\draw (4.5,1)--(5.0,1);
\draw (5,0) rectangle (6,1);
\draw node at (5.5,0.2) [above] {$A_{Q}$};
\draw (6,0) rectangle (7,1);
\draw node at (6.5,0.2) [above] {$A_{Q+1}$};
\draw (7,0) rectangle (8,1);
\draw node at (7.5,0.2) [above] {$A_{Q+2}$};
\draw (8,0) rectangle (9,1);
\draw node at (8.5,0.2) [above] {$A_{Q+3}$};
\draw (9,0) rectangle (10,1);
\draw (10,0)--(10.5,0);
\draw (10,1)--(10.5,1);
%
\draw[->] (4.0, -0.5) -- (4.0, -1.0);
%
\draw (-1.0,-2.5) rectangle (0,-1.5);
\draw node at (-0.5,-2.3) [above] {$A_{1-Q}$};
\draw (0,-2.5) rectangle (1,-1.5);
\draw node at (0.5,-2.3) [above] {$A_{2-Q}$};
\draw (1,-2.5) rectangle (2,-1.5);
\draw node at (1.5,-2.3) [above] {$A_{3-Q}$};
\draw (2,-2.5) rectangle (3,-1.5);
\draw node at (2.5,-2.3) [above] {$A_{4-Q}$};
\draw (3,-2.5)--(3.5,-2.5);
\draw (3,-1.5)--(3.5,-1.5);
\draw[loosely dotted, very thick] (3.75,-2.0)--(4.25,-2.0);
\draw (4.5,-2.5)--(5.0,-2.5);
\draw (4.5,-1.5)--(5.0,-1.5);
\draw (5,-2.5) rectangle (6,-1.5);
\draw node at (5.5,-2.3) [above] {$A_{0}$};
\draw (6,-2.5) rectangle (7,-1.5);
\draw node at (6.5,-2.3) [above] {$A_{1}$};
\draw (7,-2.5) rectangle (8,-1.5);
\draw node at (7.5,-2.3) [above] {$A_{2}$};
\draw (8,-2.5) rectangle (9,-1.5);
\draw node at (8.5,-2.3) [above] {$A_{3}$};
\draw (9,-2.5) rectangle (10,-1.5);
\draw (10,-2.5)--(10.5,-2.5);
\draw (10,-1.5)--(10.5,-1.5);
\end{tikzpicture}
\caption{Replacement of the index}
\label{fig:replace}
\end{figure}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Derivation of marginal distribution}
We consider the event of $\left< A_n=i \right>$ for $i\geq 1$. The event occurs when $n$-th block coincides $(n-i)$-th block and do not coincide other blocks between $n$-th and $(n-i)$-th blocks as illustrated in Figure \ref{fig:A_n=i}. Let $\mathcal{M}$ be such an event. Then, $\mathcal{M}$ is written as
\begin{align}
  \mathcal{M} = \left< b_{n-i} = b_{n}, b_{n-i+1} \neq b_{n} , \dots, b_{n-1} \neq b_{n}  \right>,
\end{align}
where $b_k$ is the $k$-th block of a sequence $\hat{x}^n$.
Then, we can derive the probability of occurring $\left< A_n = i \right>$ under the assumption that the blocks are statistically independent and identically distributed as
\begin{align}
  \label{eq:conditional_probability}
  \mathrm{Pr}[A_n=i] = \sum_{r=0}^{L} \mathrm{Pr}[\mathcal{M} \mid \ell(b_n) = r] \times \mathrm{Pr} [\ell(b_n) = r],
\end{align}
where $\ell(b)$ denotes the number of ``$1$'' included in the block $b \in \{0,1\}^L$. We also have
\begin{align}
  \mathrm{Pr}[\mathcal{M} \mid \ell(b_n) = r] &= w_r \times ( 1 - w_r )^{i-1},\label{eq:probability_M_mid} \\
  \mathrm{Pr} [\ell(b_n) = r] &= \binom{L}{r} w_r, \label{eq:probability_l_r}
\end{align}
where $w_r = \hat{q}^r (1-\hat{q})^{L-r}$ and $\binom{L}{r} = \frac{L!}{r!(L-r)!}$ is a binomial coefficient. 
%
By combining Eqs. (\ref{eq:conditional_probability})--(\ref{eq:probability_l_r}), the following relation is obtained as
\begin{align}\label{eq:A_n=i}
  \mathrm{Pr}[A_n=i] = \sum_{r=0}^{L} \binom{L}{r} w_r^2 ( 1 - w_r )^{i-1},
\end{align} 
for $i\geq 1$.
%
\begin{figure}
\centering
\begin{tikzpicture}
\draw (-1.5,0)--(-1.0,0);
\draw (-1.5,1)--(-1.0,1);
\draw (-1.0,0) rectangle (0,1);
\draw (0,0) rectangle (1,1);
\filldraw [draw=black, fill=pink] (1,0) rectangle (2,1) node(A) at (1.5, 1.0) [above] {$b_{n-i}$};
\filldraw [pattern = north east lines] (2,0) rectangle (3,1);
\filldraw [pattern = north east lines] (3,0) rectangle (4,1);
\filldraw [pattern = north east lines] (4,0) rectangle (5,1);
\filldraw [pattern = north east lines] (5,0) rectangle (6,1);
\filldraw [pattern = north east lines] (6,0) rectangle (7,1);
\filldraw [draw=black, fill=pink] (7,0) rectangle (8,1) node(AA) at (7.5, 1.0) [above] {$b_n$};
\draw (8,0) rectangle (9,1);
\draw (9,0) rectangle (10,1);
\draw (10,0)--(10.5,0);
\draw (10,1)--(10.5,1);
%
\draw[<->] (A) to[bend left=20] node [above] {\small $b_{n-i} = b_{n+k-j}$} (AA);
\end{tikzpicture}
\caption{The arrangement of blocks in the case of $A_n=i$.}
\label{fig:A_n=i}
\end{figure}
%
%-----------------------------------------------------------------------------------------------%
\subsection{Derivation of joint distribution}\label{subsec:4-2}
We consider the event $\left< A_n=i,\, A_{n+k}=j \right>$ for $i\geq 1$ and $j\geq 1$. 
It has been shown in \cite{coron1998accurate} that the following relation holds for a truly random sequence $R^N$
%
\begin{align}\begin{split}
  &\mathrm{Pr}[A_n(R^N)=i,\, A_{n+k}(R^N)=j] \\
   &=\left\{ \begin{array}{ll}
    2^{-2L}(1-2^{-L})^{i+j-2} & (1 \leq j \leq k-1) \\
    2^{-2L}(1-2^{-L})^{i+k-2} & (j=k) \\
    2^{-2L}(1-2^{-L})^{i-j+2k-1} \left( 1 - 2^{-L+1} \right)^{j-k-1} & (k+1 \leq j \leq k+i-1) \\
    0 & (j=k+i) \\
    2^{-2L}(1-2^{-L})^{-i+j-1} \left( 1 - 2^{-L+1} \right)^{i-1} & (j \geq k+i+1) 
  \end{array} \right..
\end{split}\end{align}
In this subsection, we derive the joint distribution of $(A_n(\hat{x}^n),A_{n+k}(\hat{x}^n))$ holding for any $\hat{q} \in (0,1)$.
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsubsection{Case of $1 \leq j \leq k-1$}
When $1 \leq j \leq k-1$, the events $\left< A_n=i \right>$ and $\left< A_{n+k}=j \right>$ are independent each other as illustrated in Figure \ref{fig:case1}, since there are no overlapping between blocks from $b_{n-i}$ to $b_{n}$ and blocks from $b_{n+k-j}$ to $b_{n+k}$. Thus, we obtain the joint distribution as
\begin{align}
\begin{split}
  \label{eq:joint1}
  \mathrm{Pr}[A_n=i, A_{n+k}=j] =& \mathrm{Pr}[A_n=i] \times \mathrm{Pr}[A_{n+k}=j]\\
  =&\left( \sum_{r=0}^{L} \binom{L}{r}w_r^2 (1-w_r)^{i-1} \right) \times \left( \sum_{r=0}^{L} \binom{L}{r}w_r^2 (1-w_r)^{j-1} \right).
  \end{split}
\end{align}
In the above relations, the second equality is obtained from Eq. (\ref{eq:A_n=i}). Recall that $w_r = \hat{q}^r(1-\hat{q})^{L-r}$ where $\hat{q} \in (0,1)$.
\begin{figure}
\centering
  \begin{tikzpicture}
    \draw (-0.3,0)--(0,0);
    \draw (-0.3,0.6)--(0,0.6);
    \draw (0,0) rectangle (0.6,0.6);
    \filldraw [draw=black, fill=pink] (0.6,0) rectangle (0.6*2, 0.6) node (A) at (0.9, 0.6) [above] {$b_{n-i}$};
    \filldraw [pattern = north east lines] (0.6*2, 0) rectangle (0.6*3, 0.6);
    \filldraw [pattern = north east lines] (0.6*3, 0) rectangle (0.6*4, 0.6);
    \filldraw [pattern = north east lines] (0.6*4, 0) rectangle (0.6*5, 0.6);
    \filldraw [draw=black, fill=pink] (0.6*5, 0) rectangle (0.6*6, 0.6) node (AA) at (3.3, 0.6) [above] {$b_{n+k-j}$};
    \filldraw [draw=black, fill=white] (0.6*6, 0) rectangle (0.6*7, 0.6);
    \filldraw [draw=black, fill=white] (0.6*7, 0) rectangle (0.6*8, 0.6);
    \filldraw [draw=black, fill=yellow!60] (0.6*8, 0) rectangle (0.6*9, 0.6) node (B) at (5.1, 0.6) [above]{$b_n$};
    \filldraw [pattern = north west lines] (0.6*9, 0) rectangle (0.6*10, 0.6);
    \filldraw [pattern = north west lines] (0.6*10, 0) rectangle (0.6*11, 0.6);
    \filldraw [pattern = north west lines] (0.6*11, 0) rectangle (0.6*12, 0.6);
    \filldraw [pattern = north west lines] (0.6*12, 0) rectangle (0.6*13, 0.6);
    \filldraw [pattern = north west lines] (0.6*13, 0) rectangle (0.6*14, 0.6);
    \filldraw [draw=black, fill=yellow!60] (0.6*14, 0) rectangle (0.6*15, 0.6) node (BB) at (8.7, 0.6) [above]{$b_{n+k}$};
    \draw (0.6*15, 0) rectangle (0.6*16, 0.6);
    \draw (0.6*16, 0)--(0.6*16.5, 0);
    \draw (0.6*16, 0.6)--(0.6*16.5, 0.6);
    %
    \draw[<->] (A) to[bend left=30] node [above] {\small $b_{n-i} = b_{n+k-j}$} (AA);
    \draw[<->] (B) to[bend left=30] node [above] {\small $b_{n} = b_{n+k}$} (BB);
  \end{tikzpicture}
  \caption{An example of the arrangement of blocks in the case of $1\leq j \leq k-1$}
  \label{fig:case1}
\end{figure}
%-----------------------------------------------------------------------------------------------%
\subsubsection{Case of $j=k$}
For every $b \in B^{L}$, we consider the event $e_2(b)=\left< A_n=i ,\, A_{n+k}=j,\,b_n=b\right>$ for $j=k$. 
An example for the arrangement of blocks is illustrated in Figure \ref{fig:case2}.
The event $e_2(b)$ can be written as
\begin{align}\label{eq:e_2}
\begin{split}
  e_2(b) 
    = &\left< b_{n-i} = b , b_{n} = b, b_{n+k} = b \right> \\
    &\land \left< b_{n-i+1} \neq b , \dots , b_{n-1} \neq b \right> \\
    &\land \left< b_{n+1} \neq b , \dots , b_{n+k-1} \neq b \right>.
\end{split}
\end{align}
Since the blocks are statistically independent and uniformly distributed, we have
\begin{align}\label{eq:probability_e_2}
  \mathrm{Pr} \left[ e_2(b) \right] 
  =w_r^3 \times (1-w_{r})^{i+k-2}.
\end{align}
%
We define $\mathcal{E}_2$ as the event of occurring $\left< A_n=i ,\, A_{n+k}=j\right>$ in the case of $j=k$. Then, $\mathcal{E}_2$ can be written by using Eq. (\ref{eq:e_2}) as
\begin{align}\label{eq:E_2}
  \mathcal{E}_2 = \bigvee_{b\in B^L} e_2(b).
\end{align}
%
Therefore, the joint distribution is derived as follows:
\begin{align}
\begin{split}\label{eq:joint_dist_2}
  \mathrm{Pr}[A_n=i ,\, A_{n+k}=j] &=
  \mathrm{Pr}[\mathcal{E}_2] \\
  &=\mathrm{Pr}\left[ \bigvee_{b \in B^L} e_2(b) \right] \\
  &= \sum_{b\in B^L} \mathrm{Pr} [e_2(b)] \\
  &= \sum_{b\in B_0^L \cup \dots \cup B_L^L} \mathrm{Pr} [e_2(b)] \\
  &= \sum_{r=0}^{L} \sum_{b\in B_{r}^L} \mathrm{Pr} \left[ e_2(b) \right] \\
  &= \sum_{r=0}^{L} \left(\sharp B_{r}^L \right) \mathrm{Pr} \left[ e_2(b) \right] \\
  &= \sum_{r=0}^{L} \dbinom{L}{r} w_{r}^3 (1-w_{r})^{i+k-2},
\end{split}
\end{align}
where $B_r^L := \{ b\in B^L \mid \ell(b)=r \}$ and $\sharp \mathcal{S}$ be the number of elements included in a finite set $\mathcal{S}$.
The sixth equality in Eq. (\ref{eq:joint_dist_2}) has been obtained with the fact that $\mathrm{Pr}[e_2(b)]$ depends only on $\ell (b)$.
%
\begin{figure}
\centering
  \begin{tikzpicture}
    \draw (-0.3,0)--(0,0);
    \draw (-0.3,0.6)--(0,0.6);
    \draw (0,0) rectangle (0.6,0.6);
    \filldraw [draw=black, fill=pink] (0.6,0) rectangle (0.6*2, 0.6) node (A) at (0.9, 0.6) [above]{$b_{n-i}$};
    \filldraw [pattern = north east lines] (0.6*2, 0) rectangle (0.6*3, 0.6);
    \filldraw [pattern = north east lines] (0.6*3, 0) rectangle (0.6*4, 0.6);
    \filldraw [pattern = north east lines] (0.6*4, 0) rectangle (0.6*5, 0.6);
    \filldraw [pattern = north east lines] (0.6*5, 0) rectangle (0.6*6, 0.6);
    \filldraw [draw=black, fill=pink] (0.6*6, 0) rectangle (0.6*7, 0.6) node (B) at (3.9, 0.6) [above]{$b_{n}$};
    \filldraw [pattern = north east lines] (0.6*7, 0) rectangle (0.6*8, 0.6);
    \filldraw [pattern = north east lines] (0.6*8, 0) rectangle (0.6*9, 0.6);
    \filldraw [pattern = north east lines] (0.6*9, 0) rectangle (0.6*10, 0.6);
    \filldraw [pattern = north east lines] (0.6*10, 0) rectangle (0.6*11, 0.6);
    \filldraw [pattern = north east lines] (0.6*11, 0) rectangle (0.6*12, 0.6);
    \filldraw [pattern = north east lines] (0.6*12, 0) rectangle (0.6*13, 0.6);
    \filldraw [pattern = north east lines] (0.6*13, 0) rectangle (0.6*14, 0.6);
    \filldraw [draw=black, fill=pink] (0.6*14, 0) rectangle (0.6*15, 0.6) node (C) at (8.7, 0.6) [above]{$b_{n+k}$};
    \draw (0.6*15, 0) rectangle (0.6*16, 0.6);
    \draw (0.6*16, 0)--(0.6*16.5, 0);
    \draw (0.6*16, 0.6)--(0.6*16.5, 0.6);
    %
    \draw[<->] (A) to[bend left=20] node [above] {\small $b_{n-i} = b_{n}$} (B);
    \draw[<->] (B) to[bend left=20] node [above] {\small $b_{n} = b_{n+k}$} (C);
  \end{tikzpicture}
  \caption{An example of the arrangement of blocks in the case of $j = k$}
  \label{fig:case2}
\end{figure}
%-----------------------------------------------------------------------------------------------%
\subsubsection{Case of $k+1 \leq j \leq k+i-1$}%case3
For every $b,\, b^\prime \in B^L$, we consider the event $e_3(b,b^\prime) \left< A_n=i ,\, A_{n+k}=j,\,b_n=b,\,b_{n+k}=b^\prime\right>$ for $k+1 \leq j \leq k+i-1$. 
An example for the arrangement of blocks is illustrated in Figure \ref{fig:case3}.
The event $e_3(b,b^\prime)$ can be written as
\begin{align}
\begin{split}
  \label{eq:e_3}
  e_3 \left(b,b^\prime\right) = 
  &\left< b_{n-i} = b , b_{n} = b , b_{n+k-j} = b^\prime , b_{n+k} = b^\prime \right> \\
  &\land \left< b_{n-i+1} \neq b, \dots, b_{n+k-j-1} \neq b \right> \\
  &\land \left< b_{n+k-j+1} \neq b, \dots, b_{n-1} \neq b \right> \\
  &\land \left< b_{n+k-j+1} \neq b^\prime, \dots, b_{n-1} \neq b^\prime \right> \\
  &\land \left< b_{n+1} \neq b^\prime , \dots, b_{n+k-1} \neq b^\prime \right>.
\end{split}
\end{align}
Since the blocks are statistically independent and uniformly distributed, we have
\begin{align}
\begin{split}
  \label{eq:probability_e3}
  \mathrm{Pr} \left[ e_3 \left(b,b^\prime\right) \right] 
  =& w_{r_1}^2 w_{r_2}^2 
  (1-w_{r_1})^{i-j+k-1} 
  (1-w_{r_1}-w_{r_2})^{j-k-1}
  (1-w_{r_2})^{k-1}.
\end{split}
\end{align}
We define $\mathcal{E}_3$ as the event $\left< A_n=i ,\, A_{n+k}=j \right>$ in the case of $k+1 \leq j \leq k+i-1$, $\mathcal{E}_3$ can be written by using $e_3(b,b^\prime)$ as
\begin{align}\label{eq:E_3}
  \mathcal{E}_3 = \bigvee_{b \in B^L} \bigvee_{b^\prime \in B^L \setminus \{b\}} e_3 \left(b,b^\prime\right).
\end{align}
Therefore, we can derive the joint distribution as
\begin{align}\label{eq:joint_dist_3}
\begin{split}
  &\mathrm{Pr} [A_n=i ,\, A_{n+k}=j] \\
  &=\mathrm{Pr} [\mathcal{E}_3] \\
  &= \mathrm{Pr} \left[ \bigvee_{b_1 \in B^L} \bigvee_{b_2 \in B^L \setminus \{b_1\}}
  e_3(b,b^\prime) \right] \\
  %
  &=\sum_{b_1 \in B^L} \sum_{b_2 \in B^{L} \setminus \{ b_1 \}} \mathrm{Pr} \left[ e_3(b,b^\prime) \right] \\
  %
  &= \sum_{r_1=0}^{L} \sum_{b_1 \in B^L_{r_1}} \sum_{r_2=0}^{L} \sum_{b_2 \in B^L_{r_2} \setminus \{ b_1 \}} \mathrm{Pr} \left[ e_3(b,b^\prime) \right] \\
  %
  &= \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \sum_{b_1 \in B^L_{r_1}} \sum_{b_2 \in B^L_{r_2}} \mathrm{Pr} \left[e_3(b,b^\prime) \right] 
  %
  + \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \sum_{b_1 \in B^L_{r_1}} \sum_{b_2 \in B^L_{r_1} \setminus \{ b_1 \}} \mathrm{Pr} \left[e_3(b,b^\prime) \right] \\
  %
  &= \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \dbinom{L}{r_1} \dbinom{L}{r_2} \mathrm{Pr} \left[e_3(b,b^\prime) \right] 
  %
  + \sum_{r_1=0}^{L}\sum_{r_2\in\{r_1\}} \dbinom{L}{r_1} \left\{ \dbinom{L}{r_1} -1 \right\} \mathrm{Pr} \left[e_3(b,b^\prime) \right],
\end{split}
\end{align}
where $\mathrm{Pr} \left[e_3(b,b^\prime) \right]$ is given in Eq. (\ref{eq:probability_e3}).
The last equality in Eq. (\ref{eq:joint_dist_3}) holds since $\mathrm{Pr} [A_n=i ,\, A_{n+k}=j,\,b_n=b,\,b_{n+k}=b^\prime]$ depends only on $\ell (b)$ and $\ell (b^\prime)$.
%
\begin{figure}
\centering
  \begin{tikzpicture}
    \draw (-0.3,0)--(0,0);
    \draw (-0.3,0.6)--(0,0.6);
    \draw (0,0) rectangle (0.6,0.6);
    \filldraw [draw=black, fill=pink] (0.6,0) rectangle (0.6*2, 0.6) node (A) at (0.9,0.6) [above]{$b_{n-i}$};
    \filldraw [pattern = north east lines] (0.6*2, 0) rectangle (0.6*3, 0.6);
    \filldraw [pattern = north east lines] (0.6*3, 0) rectangle (0.6*4, 0.6);
    \filldraw [draw=black, fill=yellow!60] (0.6*4, 0) rectangle (0.6*5, 0.6) node (B) at (2.7,0.6) [above] {$b_{n+k-j}$};
    \filldraw [pattern = crosshatch] (0.6*5, 0) rectangle (0.6*6, 0.6);
    \filldraw [pattern = crosshatch] (0.6*6, 0) rectangle (0.6*7, 0.6);
    \filldraw [pattern = crosshatch] (0.6*7, 0) rectangle (0.6*8, 0.6);
    \filldraw [draw=black, fill=pink] (0.6*8, 0) rectangle (0.6*9, 0.6)node (AA) at (5.1,0.6)[above] {$b_n$};
    \filldraw [pattern = north east lines] (0.6*9, 0) rectangle (0.6*10, 0.6);
    \filldraw [pattern = north east lines] (0.6*10, 0) rectangle (0.6*11, 0.6);
    \filldraw [pattern = north east lines] (0.6*11, 0) rectangle (0.6*12, 0.6);
    \filldraw [pattern = north east lines] (0.6*12, 0) rectangle (0.6*13, 0.6);
    \filldraw [pattern = north east lines] (0.6*13, 0) rectangle (0.6*14, 0.6);
    \filldraw [draw=black, fill=yellow!60] (0.6*14, 0) rectangle (0.6*15, 0.6)node (BB) at (8.7,0.6)[above]{$b_{n+k}$};
    \draw (0.6*15, 0) rectangle (0.6*16, 0.6);
    \draw (0.6*16, 0)--(0.6*16.5, 0);
    \draw (0.6*16, 0.6)--(0.6*16.5, 0.6);
    %
    \draw[<->] (A) to[bend left=20] node [above] {\small $b_{n-i} = b_{n}$} (AA);
    \draw[<->] (B) to[bend left=20] node [above] {\small $b_{n+k-j} = b_{n+k}$} (BB);
  \end{tikzpicture}
  \caption{An example of the arrangement of blocks in the case of $k+1 \leq j \leq k+i-1$}
  \label{fig:case3}
\end{figure}
%-----------------------------------------------------------------------------------------------%
\subsubsection{Case of $j=k+i$}%case4
When $j=k+i$, the events $\left< A_n=i \right>$ and $\left< A_{n+k}=j \right>$ do not occur coincidentally as illustrated in Figure \ref{fig:case4}. Thus, the joint distribution is obtained as
\begin{align}
  \label{eq:joint4}
  \mathrm{Pr}[A_n=i,A_{n+k}=j] = 0.
\end{align}
%
\begin{figure}
\centering
\begin{tikzpicture}
  \draw (-0.3,0)--(0,0);
  \draw (-0.3,0.6)--(0,0.6);
  \draw (0,0) rectangle (0.6,0.6);
  \filldraw [draw=black, fill=pink] (0.6,0) rectangle (0.6*2, 0.6) node (A) at (0.9,0.6)[above]{$b_{n-i}$};
  \filldraw [pattern = north east lines] (0.6*2, 0) rectangle (0.6*3, 0.6);
  \filldraw [pattern = north east lines] (0.6*3, 0) rectangle (0.6*4, 0.6);
  \filldraw [pattern = north east lines] (0.6*4, 0) rectangle (0.6*5, 0.6);
  \filldraw [pattern = north east lines] (0.6*5, 0) rectangle (0.6*6, 0.6);
  \filldraw [pattern = north east lines] (0.6*6, 0) rectangle (0.6*7, 0.6);
  \filldraw [pattern = north east lines] (0.6*7, 0) rectangle (0.6*8, 0.6);
  \filldraw [draw=black, fill=pink] (0.6*8, 0) rectangle (0.6*9, 0.6) node (B) at (5.1,0.6)[above]{$b_{n}$};
  \filldraw [pattern = north east lines] (0.6*9, 0) rectangle (0.6*10, 0.6);
  \filldraw [pattern = north east lines] (0.6*10, 0) rectangle (0.6*11, 0.6);
  \filldraw [pattern = north east lines] (0.6*11, 0) rectangle (0.6*12, 0.6);
  \filldraw [pattern = north east lines] (0.6*12, 0) rectangle (0.6*13, 0.6);
  \filldraw [pattern = north east lines] (0.6*13, 0) rectangle (0.6*14, 0.6);
  \filldraw [draw=black, fill=pink] (0.6*14, 0) rectangle (0.6*15, 0.6)node (C) at (8.7,0.6)[above]{$b_{n+k}$};
  \draw (0.6*15, 0) rectangle (0.6*16, 0.6);
  \draw (0.6*16, 0)--(0.6*16.5, 0);
  \draw (0.6*16, 0.6)--(0.6*16.5, 0.6);
  %
  \draw[<->] (A) to[bend left=10] node [above] {\small $b_{n-i} = b_{n}$} (B);
  \draw[<->] (B) to[bend left=10] node [above] {\small $b_{n} \neq b_{n+k}$} (C);
  \draw[<->, dashed] (A) to[bend left=30] node [above] {\small $b_{n-i} = b_{n+k}$} (C);
\end{tikzpicture}
  \caption{An example of the arrangement of blocks in the case of $j=k+i$}
  \label{fig:case4}
\end{figure}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsubsection{Case of $j \geq k+i+1$}
For every $b,\, b^\prime \in B^L$, we consider the event $\left< A_n=i ,\, A_{n+k}=j,b_n=b,\,b_{n+k}=b^\prime\right>$ occurs when $j \geq k+i+1$. An example for the arrangement of blocks is illustrated in Figure \ref{fig:case5}. Let $e_5(b_1,b_2)$ be the event of occurring $\left< A_n=i ,\, A_{n+k}=j,b_n=b,\,b_{n+k}=b^\prime\right>$ which is written as
\begin{align}
\begin{split}
  \label{eq:e_5}
  e_5 (b,b^\prime) := 
  &\left< b_{n+k-j} = b , b_{n-i} = b^\prime , b_{n} = b^\prime , b_{n+k} = b \right> \\
  &\land \left< b_{n+k-j+1} \neq b, \dots, b_{n-i-1} \neq b \right> \\
  &\land \left< b_{n-i+1} \neq b, \dots, b_{n-1} \neq b \right> \\
  &\land \left< b_{n-i+1} \neq b^\prime, \dots, b_{n-1} \neq b^\prime \right> \\
  &\land \left< b_{n+1} \neq b , \dots, b_{n+k-1} \neq b \right>.
\end{split}
\end{align}
Since the blocks are statistically independent and uniformly distributed, we have
\begin{align}
\begin{split}
  \label{eq:probability_e5}
  \mathrm{Pr} \left[ e_5(b_1,b_2) \right] 
  =& w_{r_1}^2  w_{r_2}^2 
  (1-w_{r_1})^{-i+j-k-1} 
  (1-w_{r_1}-w_{r_2})^{i-1}
  (1-w_{r_2})^{k-1}. 
\end{split}
\end{align}
We define $\mathcal{E}_5$ as the event $\left< A_n=i ,\, A_{n+k}=j\right>$ for $j \geq k+i+1$. Then, $\mathcal{E}_5$ can be written by using $e_5(b_1,b_2)$ as
\begin{align}\label{eq:E_5}
  \mathcal{E}_5 = \bigvee_{b \in B^L} \bigvee_{b^\prime \in B^L \setminus \{b_1\}} e_5(b,b^\prime).
\end{align}
%
Therefore, we can derive the joint distribution as
\begin{align}\label{eq:joint_dist_5}
\begin{split}
  &\mathrm{Pr} [A_n=i,\, A_{n+k}=j] \\
  &=\mathrm{Pr} [\mathcal{E}_5] \\
  &= \mathrm{Pr} \left[ \bigvee_{b \in B^L} \bigvee_{b^\prime \in B^L \setminus \{b\}}
  e_5(b,b^\prime) \right] \\
  %
  &=\sum_{b \in B^L} \sum_{b^\prime \in B^{L} \setminus \{ b \}} \mathrm{Pr} \left[ e_5(b,b^\prime) \right] \\
  %
  &= \sum_{r_1=0}^{L} \sum_{b \in B^L_{r_1}} \sum_{r_2=0}^{L} \sum_{b^\prime \in B^L_{r_2} \setminus \{ b \}} \mathrm{Pr} \left[ e_5(b,b^\prime) \right] \\
  %
  &= \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \sum_{b \in B^L_{r_1}} \sum_{b^\prime \in B^L_{r_2}} \mathrm{Pr} \left[e_5(b_1,b_2) \right] 
  %
  + \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \sum_{b \in B^L_{r_1}} \sum_{b^\prime \in B^L_{r_1} \setminus \{ b \}} \mathrm{Pr} \left[e_5(b,b^\prime) \right] \\
  %
  &= \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \dbinom{L}{r_1} \dbinom{L}{r_2} \mathrm{Pr} \left[e_5(b,b^\prime) \right] 
  %
  + \sum_{r_1=0}^{L}\sum_{r_2\in\{r_1\}} \dbinom{L}{r_1} \left\{ \dbinom{L}{r_1} -1 \right\} \mathrm{Pr} \left[e_5(b,b^\prime) \right],
\end{split}
\end{align}
where $\mathrm{Pr} \left[e_5(b_1,b_2) \right]$ is obtained in Eq. (\ref{eq:probability_e5}).
The above relations have been obtained in the same manner in the case of $k+1 \leq j \leq k+i-1$.
%
\begin{figure}
\centering
  \begin{tikzpicture}
    \draw (-0.3,0)--(0,0);
    \draw (-0.3,0.6)--(0,0.6);
    \draw (0,0) rectangle (0.6,0.6);
    \filldraw [draw=black, fill=yellow!60] (0.6,0) rectangle (0.6*2, 0.6)node (A) at (0.9,0.6) [above]{$b_{n+k-j}$};
    \filldraw [pattern = north east lines] (0.6*2, 0) rectangle (0.6*3, 0.6);
    \filldraw [pattern = north east lines] (0.6*3, 0) rectangle (0.6*4, 0.6);
    \filldraw [draw=black, fill=pink] (0.6*4, 0) rectangle (0.6*5, 0.6)node (B) at (2.7,0.6) [above]{$b_{n-i}$};
    \filldraw [pattern = crosshatch] (0.6*5, 0) rectangle (0.6*6, 0.6);
    \filldraw [pattern = crosshatch] (0.6*6, 0) rectangle (0.6*7, 0.6);
    \filldraw [pattern = crosshatch] (0.6*7, 0) rectangle (0.6*8, 0.6);
    \filldraw [pattern = crosshatch] (0.6*8, 0) rectangle (0.6*9, 0.6);
    \filldraw [pattern = crosshatch] (0.6*9, 0) rectangle (0.6*10, 0.6);
    \filldraw [draw=black, fill=pink] (0.6*10, 0) rectangle (0.6*11, 0.6) node (BB) at (6.3,0.6)[above]{$b_n$};
    \filldraw [pattern = north east lines] (0.6*11, 0) rectangle (0.6*12, 0.6);
    \filldraw [pattern = north east lines] (0.6*12, 0) rectangle (0.6*13, 0.6);
    \filldraw [pattern = north east lines] (0.6*13, 0) rectangle (0.6*14, 0.6);
    \filldraw [draw=black, fill=yellow!60] (0.6*14, 0) rectangle (0.6*15, 0.6)node (AA) at (8.7,0.6)[above]{$b_{n+k}$};
    \draw (0.6*15, 0) rectangle (0.6*16, 0.6);
    \draw (0.6*16, 0)--(0.6*16.5, 0);
    \draw (0.6*16, 0.6)--(0.6*16.5, 0.6);
    %
    \draw[<->] (A) to[bend left=25] node [above] {\small $b_{n+k-j} = b_{n+k}$} (AA);
    \draw[<->] (B) to[bend left=15] node [above] {\small $b_{n-i} = b_{n}$} (BB);
  \end{tikzpicture}
  \caption{An example of the arrangement of blocks in the case of $j \geq k+i+1$}
  \label{fig:case5}
\end{figure}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsubsection{Summary of the results}
%これまでのサブセクションで系列Aに関する周辺分布と同時分布について導出してきた．周辺分布は，An=iである事象を考えることによって導出され，式（）で与えられる，一方，同時分布はブロックの配置によって5つの場合に分けて解析する必要があった．ここに，結果を再掲する．
In the previous subsections, we have derived the marginal distribution of $A_n$ and the joint distribution of $(A_n,A_{n+k})$ under the assumption of $Q\to\infty$. 
We give the summary of the result of the joint distribution that we have obtained in subsection \ref{subsec:4-2}.
The joint distribution of $(A_n,A_{n+k})$ is written as follows:
\begin{align}\begin{split}\label{eq:joint_distribution}
  &\mathrm{Pr}[A_n=i,\,A_{n+k}=j] \\
  %%%---case1---
  &= \left\{ \begin{array}{ll}
  % \displaystyle\left( \sum_{r=0}^{L} \binom{L}{r}\mathcal{Q}_r^2 (1-\mathcal{Q}_r)^{i-1} \right) \times \left( \sum_{r=0}^{L} \binom{L}{r}\mathcal{Q}_r^2 (1-\mathcal{Q}_r)^{j-1} \right) & (1\leq j \leq k-1)\\
  \displaystyle\left( \sum_{r=0}^{L} \binom{L}{r}w_r^2 (1-w_r)^{i-1} \right) \times \left( \sum_{r=0}^{L} \binom{L}{r}w_r^2 (1-w_r)^{j-1} \right) & (1\leq j \leq k-1)\\
  %%%---case2---
  % \displaystyle\sum_{r=0}^{L} \dbinom{L}{r} \mathcal{Q}_{r}^3 (1-\mathcal{Q}_{r})^{i+k-2} & (j=k) \\
  \displaystyle\sum_{r=0}^{L} \dbinom{L}{r} w_{r}^3 (1-w_{r})^{i+k-2} & (j=k) \\
  %%%---case3---
  \displaystyle\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \dbinom{L}{r_1} \dbinom{L}{r_2} \mathrm{Pr} \left[e_3(b,b^\prime) \right] \\
  %
  + \displaystyle\sum_{r_1=0}^{L}\sum_{r_2\in\{r_1\}} \dbinom{L}{r_1} \left\{ \dbinom{L}{r_1} -1 \right\} \mathrm{Pr} \left[e_3(b,b^\prime) \right] & (k+1 \leq j \leq k+i-1)\\
  %%%---case4---
  0 & (j=k+i) \\
  %%%---case5---
  \displaystyle\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \dbinom{L}{r_1} \dbinom{L}{r_2} \mathrm{Pr} \left[e_5(b,b^\prime) \right] \\
  %
  \displaystyle+ \sum_{r_1=0}^{L}\sum_{r_2\in\{r_1\}} \dbinom{L}{r_1} \left\{ \dbinom{L}{r_1} -1 \right\} \mathrm{Pr} \left[e_5(b,b^\prime) \right] & (j \geq k+i+1)
  \end{array}\right.,
\end{split}\end{align}
where $w_r=\hat{q}^r(1-\hat{q})^{L-r}$. In Eq.(\ref{eq:joint_distribution}), $\mathrm{Pr} \left[e_3(b,b^\prime) \right]$ and $\mathrm{Pr} \left[e_5(b,b^\prime) \right]$ are given in Eqs. (\ref{eq:probability_e3}) and (\ref{eq:probability_e5}), respectively.
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\newpage
\section{The variance of references distribution}\label{sec:3}
In Section \ref{sec:distribution}, we have obtained the marginal distribution of $A_n$ and the joint distribution of $(A_n,\,A_{n+k})$ for any $\hat{q}\in (0,1)$.
%
In this section, we provide a theoretical deviation for the variance of reference distribution of the highly sensitive test under the null hypothesis using the results given in Section \ref{sec:distribution}.
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Theoretical derivation of the variance}
The variance of a random variable $X$ is defined by
\begin{align}
\begin{split}\label{eq:var_def}
  \mathrm{Var}[X] &= \mathbb{E}[(X - \mathbb{E}[X])^2] \\
  &=\mathbb{E}[X^2] - (\mathbb{E}[X])^2,
\end{split}
\end{align}
where $\mathbb{E}[X]$ is the expected value of $X$ and $\mathrm{Var}[X]$ is the variance of $X$.
In general, for any random variables $X_1,X_2,\dots,X_n$, the variance of the sum of $n$ variables is obtained by
\begin{align}\label{eq:sum_var}
  \mathrm{Var}\left[\sum_{i=1}^{n} X_i \right] = \sum_{i=1}^{n} \mathrm{Var}[X_i] + 2 \sum_{1\leq i < j \leq n}\mathrm{Cov}[X_i, X_j],
\end{align}
where $\mathrm{Cov}[X,Y]$ is the covariance defined by
\begin{align}\label{eq:cov_def}
  \mathrm{Cov}[X, Y] = \mathbb{E}[XY] - \mathbb{E}[X] \times \mathbb{E}[Y].
\end{align}
Let $\sigma_{C,\hat{q}}(K)^2$ be the variance for the reference distribution of the highly sensitive test with $\hat{q}$.
Using Eq. (\ref{eq:sum_var}), $\sigma_{C,\hat{q}}(K)^2$ is written as
\begin{align}
\begin{split}\label{eq:var_fC}
  \sigma_{C,\hat{q}}(K)^2 =& \mathrm{Var} [f_C(\hat{x}^n)] \\
  =& \mathrm{Var} \left[ \frac{1}{K} \sum_{n=1}^{K} g(A_n) \right] \\
  =& \frac{1}{K^2} \left( \sum_{n=1}^{K} \mathrm{Var} [g(A_n)] + 2 \sum_{1 \leq i < j \leq K} \mathrm{Cov} [g(A_{i}), g(A_{j})] \right) \\
  =& \frac{1}{K^2} \Biggl( K \times \mathrm{Var} [g(A_n)] + 2\sum_{k=1}^{K-1}(K-k) \times \mathrm{Cov}[g(A_n),g(A_{n+k})] \Biggr).
\end{split}
\end{align}
The last equality in Eq. (\ref{eq:var_fC}) has been obtained with the fact that the sequence of $\{A_k\}_{k=1}^{K}$ is stationary ergodic under the assumption $Q\to\infty$.
\par
In the next place, we derive $\mathrm{Var} [g(A_n)]$ and $\mathrm{Cov}[g(A_n),g(A_{n+k})]$ in Eq. (\ref{eq:var_fC}). First, $\mathrm{Var} [g(A_n)]$ can be written as
\begin{align}\label{eq:var_g_def}
  \mathrm{Var}[g(A_n)] 
  &= \mathbb{E}[\{g(A_n)\}^2] - (\mathbb{E}[g(A_n)])^2 
\end{align}
by Eq. (\ref{eq:var_def}). From the definition of expected value, the first term of the right hand side of Eq. (\ref{eq:var_g_def}) can be calculated as
\begin{align}\begin{split}\label{eq:expectation_g_An_square}
  \mathbb{E}[\{g(A_n)\}^2] &= \sum_{i=1}^{\infty} \{g(i)\}^2 \mathrm{Pr}[A_n=i]\\
  &=\sum_{i=2}^{\infty} \left[\left\{ (\log_2 \mathrm{e}) \sum_{k=1}^{i-1} \frac{1}{k} \right\}^2 \times \sum_{r=0}^{L} \binom{L}{r} w_r^2 (1-w_r)^{i-1}\right]. 
\end{split}\end{align}
The second term of the right hand side of Eq. (\ref{eq:var_g_def}) is equal to $\{L \times H(\hat{q})\}^2$ from Eq. (\ref{eq:E_BMS}).
%
Secondly, $\mathrm{Cov}[g(A_n),g(A_{n+k})]$ in Eq. (\ref{eq:var_fC}) can be calculated as
\begin{align}\label{eq:covariance_g_g}
\begin{split}
  \mathrm{Cov}[g(A_n),g(A_{n+k})] 
  &= \mathbb{E}[g(A_n) g(A_{n+k})] - \mathbb{E}[g(A_n)]\times\mathbb{E}[g(A_{n+k})] \\
  &= \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}g(i)g(j)\mathrm{Pr}[A_n=i, \, A_{n+k}=j] - \left\{L \times H(\hat{q})\right\}^2
\end{split}
\end{align}
where $\mathrm{Pr}[A_n=i, \, A_{n+k}=j]$ has been obtained in Eq. (\ref{eq:joint_distribution}).
The first equality in Eq. (\ref{eq:covariance_g_g}) has been obtained from Eq. (\ref{eq:cov_def}). The second equality in Eq. (\ref{eq:covariance_g_g}) has been obtained from the definition of the expected value and Eq. (\ref{eq:E_BMS}).
\par
By combining Eqs. (\ref{eq:var_fC})--(\ref{eq:covariance_g_g}), $\sigma_{C,q}(K)^2$ is rewritten as
\begin{align}\begin{split}\label{eq:sigma_C^2}
  \sigma_{C,\hat{q}}(K)^2  
  =& \frac{1}{K} \left( \sum_{i=2}^{\infty} \left[\left\{ (\log_2 \mathrm{e}) \sum_{k=1}^{i-1} \frac{1}{k} \right\}^2 \times \sum_{r=0}^{L} \binom{L}{r} w_r^2 (1-w_r)^{i-1}\right] - \{L \times H(\hat{q})\}^2 \right)\\
  &+ \frac{2}{K^2}\sum_{k=1}^{K-1}(K-k) \left\{\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}g(i)g(j)\mathrm{Pr}[A_n=i, \, A_{n+k}=j] - \left\{L \times H(\hat{q})\right\}^2\right\}.
\end{split}\end{align}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\clearpage
\subsection{Numerical results}\label{subsec:numerical_exp_L4}
In this subsection, we show some results of experiments. 
In the following, we approximate the infinite double sum as finite sum when we compute $\sigma_{C,\hat{q}} (K)^2$ by Eq. (\ref{eq:sigma_C^2}) since it is unable to compute an infinite summation by computer. Even if computing infinite sum is inevitable, we explore the covariance given in (\ref{eq:covariance_g_g}) to calculate the value more efficiently by computational experiment in Appendix \ref{appendix:B}.
%
\subsubsection{Experiment 1}
%
We confirm that $\sigma_{C,\hat{q}} (K)^2$ can be computed accurately by Eq. (\ref{eq:sigma_C^2}) when $L=4$. In the numerical computation, we set $Q=10 \times 2^L$.
Figure \ref{fig:1} shows the result of computed variance of reference distribution when $\hat{q}=0.33,\, 0.4$ and $0.5$. Note that we set $\hat{q}=0.33$ since it is said to be optimal for detecting the deviation of a binary sequence in \cite{yamamoto2016highly}. As seen in the Figure \ref{fig:1}, the variance decreases by $\mathcal{O}(\frac{1}{K})$. We can express the variance as $\frac{D_K(\hat{q})}{K}$. Table \ref{tab:1} shows the coefficient $D_K(\hat{q})$ when we approximate variance by $\frac{D_K(\hat{q})}{K}$. 
\par
In the next place, we show the result of the numerical experiment for computing an unbiased variance of binary sequences generated by a pseudo random generator. The procedure of the experiment is as follows.
\begin{enumerate}[Step1:]
  \item Set $L,\,Q,\,K,\,\hat{q},\,M$ and $N$.
  \item Generate $M$ pieces of binary sequences $x^{n,1},\dots x^{n,M}$ by pseudo random number generator, where $x^{n,i}$ for $i=1,2,\dots,M$ is a binary sequence of length $n=L\times(Q+K)$.
  \item Convert each binary sequence $x^{n,i}$ into $\hat{x}^{n,i}$ with $\hat{q}$ from Eq. (\ref{eq:convert}) by using pseudo random number generator.
  \item For each converted binary sequence $\hat{x}^{n,i}$, compute the test statistical value $f_i=f_C(\hat{x}^{n,i})$ from Eq. (\ref{eq:fC}).
  \item Compute an unbiased variance defined by
  \begin{align}
    u^2 = \frac{1}{M-1}\sum_{i=1}^{M}(f_i - \overline{f})^2,
  \end{align}
  where $\overline{f}$ is the arithmetic mean of $f_1,f_2,\dots,f_M$.
  \item Repeat Step2 to Step4 in $N$ times, and obtain $N$ unbiased variances $u_1^2,u_2^2,\dots,u_N^2$. Then, compute the arithmetic mean value of unbiased variances by
  \begin{align}
    \overline{u}^2 = \frac{1}{N} \sum_{i=1}^{N} u_i^2.
  \end{align}
\end{enumerate}
%
In the numerical simulation, we set $L=4,\,Q=10\times 2^L,\,\hat{q}=0.33,\, M=1000$ and $N=30$. We also set $K$ as $10^3,\,2\times 10^3 ,\, 4\times 10^3,\, 6\times 10^3,\, 8\times 10^3,\, 10\times 10^3,\,12\times 10^3,\,14\times 10^3$ and $16 \times 10^3$. We used Mersenne Twister as the pseudo random number generator in Step2 and Step3.
Figure \ref{fig:2} shows the result of the experiment, and we can confirm that the simulated unbiased variance coincides with the result obtained by Eq. (\ref{eq:sigma_C^2}) in precisely.
%
\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.8\linewidth]{./figure/fig3.pdf}
    \caption{The variance of the reference distribution computed based on Eq. (\ref{eq:sigma_C^2}) with $\hat{q}=0.33,\, 0.4$ and $0.5$ (Copyright(C)2020 IEICE, \cite{hikima2020} Figure 1)}
    \label{fig:1}
\end{figure}
%
\begin{table}[htbp]
  \centering
  \caption{$D_K(\hat{q})$ for different values of $\hat{q}$ and $K$}
  \begin{tabular}{ccccc} \hline
    $\hat{q}$ & $D_{10000}(\hat{q})$ & $D_{20000}(\hat{q})$ & $D_{30000}(\hat{q})$ & $D_{40000}(\hat{q})$  \\ \hline 
    $0.33$    & $1.867364$     & $1.865492$     & $1.864868$     & $1.864556$\\
    $0.4$     & $1.328692$     & $1.327430$     & $1.327009$     & $1.326799$\\
    $0.5$     & $1.028395$     & $1.027449$     & $1.027134$     & $1.026976$\\ \hline
  \end{tabular}
  \label{tab:1}
\end{table}
%
\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\linewidth]{./figure/fig4.pdf}
    \caption{The variance of the reference distribution computed in the experiment. The solid blue line and the broken red line are the variances computed based on Eq. (\ref{eq:sigma_C^2}) with $\hat{q}=0.5$ and $\hat{q}=0.33$, respectively. The block points show the arithmetic mean of the unbiased variance with $\hat{q}=0.33$ using Mersenne Twister. (Copyright(C)2020 IEICE, \cite{hikima2020} Figure 2)}
    \label{fig:2}
\end{figure}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\clearpage
\subsubsection{Experiment 2}\label{subsec:4-3}
We have seen that the variance of reference distribution of the highly sensitive test can be computed when $L=4$. We consider the case of $L=8$ and $\hat{q}=0.33$ which are recommended in \cite{yamamoto2016highly}. 
However, the computational cost for $L=8$ is too high to compute directly since the recommendation value for $K$ is $1000\times 2^8 = 256000$.
To overcome the obstacle, we derive the fitted curve. 
We approximate the variance of the reference distribution as
\begin{align}\label{eq:approx_sigma}
  \sigma_{C,\hat{q}}(K)^2 = \frac{1}{K} \left( a + \frac{b}{K} \right),
\end{align}
where $a$ and $b$ are real valued constants. 
These constants can be obtained with any two points $(K_1, \sigma_{C,\hat{q}}(K_1)^2)$ and $(K_2, \sigma_{C,\hat{q}}(K_2)^2)$ by
\begin{align}\begin{split}\label{eq:keisuu_ab}
  a &= \frac{1}{K_1-K_2} \left( K_1^2 \sigma_{C,\hat{q}}(K_1)^2 - K_2^2 \sigma_{C,\hat{q}}(K_2)^2  \right), \\
  b &= \frac{K_1K_2}{K_2-K_1} \left( K_1 \sigma_{C,\hat{q}}(K_1)^2 - K_2 \sigma_{C,\hat{q}}(K_2)^2 \right).
\end{split}\end{align}
%
Table \ref{tab:2} shows the pairs of $(a,b)$ obtained from $(K_1,K_2)=(40000,45000)$ and the standard deviation $\tilde{\sigma}_{C,\hat{q}}(K)$ for $K=1000\times2^L$ obtained from Eq. (\ref{eq:approx_sigma}).
%
Figure \ref{eq:approx_sigma} show the fitted curve.
Using the fitted curve, we obtained the standard deviation for $K=1000\times 2^8$ as
\begin{align}\label{eq:proposed_value}
  \sigma_{C,0.33} (1000\times 2^8) = 0.003488600339.
\end{align}
%
%
\par
To confirm the accuracy of Eq. (\ref{eq:proposed_value}), we computed $\sigma_{C,0.33} (1000\times 2^8)$ using MT in 10 times. For each trial, we used $4\times 10^6$ pieces of binary sequences and set $Q=10\times 2^8$.
Table \ref{tab:2} and Figure \ref{fig:comparison_yamamoto} show the results of this experiment.
These results support that the value represented in Eq. (\ref{eq:proposed_value}) is more accurate than the value in previous study.
%
\begin{table}[htb]
  \centering
  \caption{The pairs of $(a,b)$ when $(K_1,K_2)=(40000,45000)$ and the standard deviation obtained from Eq. (\ref{eq:approx_sigma})}
  \begin{tabular}{ccc} \hline
    $(K_1,K_2)$      & $(a,b)$                            & $\tilde{\sigma}_{C,q}(K)$   \\ \hline 
    % $(20000,30000)$  & $(3.112120333856, 897.0609512838)$ & $0.003488611201$      \\
    % $(30000,40000)$  & $(3.112100900335, 897.6439269103)$ & $0.003488601597$      \\
    $(40000,45000)$  & $(3.112098237555, 897.7504381251)$ & $0.003488600339$      \\ \hline
    % $(45000,46000)$  & $(3.112097857417, 897.7675443506)$ & $0.003488600164$      \\ \hline
  \end{tabular}
  \label{tab:2}
\end{table}
%
\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\linewidth]{./figure/approx_varf_L8_10000.pdf}
    \caption{The fitted curve}
    \label{fig:fitted}
\end{figure}
%
%-----------------------------------------------------------------------------------------------%
%
\begin{table}[tbp]
  \centering
  \caption{Value of $\sigma_{C,0.33}(1000\times 2^8)$ computed using the Mersenne Twister}
  \begin{tabular}{cc} \hline
    Trial No.   & $\sigma_{C,0.33}(1000\times 2^8)$           \\ \hline 
    1           & $0.00348911$       \\
    2           & $0.00348889$       \\
    3           & $0.00348837$       \\ 
    4           & $0.00349002$       \\ 
    5           & $0.00348612$       \\ 
    6           & $0.00348572$       \\ 
    7           & $0.00348889$       \\ 
    8           & $0.00348767$       \\ 
    9           & $0.00348672$       \\ 
    10          & $0.00349002$       \\ \hline 
    Total       & $0.00348816 \pm 2.44 \times 10^{-6}$ \\ \hline
  \end{tabular}
  \label{tab:3}
\end{table}
%
\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.7\linewidth]{./figure/unbiased_variance.pdf}
  \caption{The variance $\sigma_{C,0.33}(1000\times 2^8)$. Each point shows an unbiased variance derived empirically and the red broken line shows the arithmetic mean. The blue broken line shows the value given in \cite{yamamoto2016highly}. The black broken line represented in Eq. (\ref{eq:proposed_value}).}
  \label{fig:comparison_yamamoto}
\end{figure}
%-----------------------------------------------------------------------------------------------%
\clearpage
\subsubsection{Experiment 3}
We investigated the difference between the value derived in Experiment 2 and the value given in \cite{yamamoto2016highly}. In the follows, we refer to the value represented in Eq. (\ref{eq:proposed_value}) as proposed value and the value given in \cite{yamamoto2016highly} as Yamamoto's value. We used MT \cite{matsumoto1998mersenne} and AES-128 CTR \cite{rijmen2001advanced} as pseudo random number generators. 
\par
Each test was performed for $10^5$ sequences of length $n=2068480$-bit. We divided them into $100$ sets of $1000$ sequences. We assigned pass or rejected for each set based on two-level tests, ``proportion test'' and ``uniformity test'', described in subsection \ref{subsec:1-2}.
%
Recall that the significance level for uniformity test is $0.0001$. 
Since the significance level for uniformity test is so small that we cannot observe the difference, we executed uniformity test with the significance levels $0.01$ and $0.05$.
By the same reason, we additionally performed proportion test with $\xi=2$ as well as $\xi=3$.
\par
The results of Experiment 2 imply that more sequences should be used to observe the differences between the highly sensitive test with the proposed value and the test with the Yamamoto's value. We cannot expect to get any meaningful results because there are some approximation errors. For instance, we assume that the p-value takes any real value in $[0,1]$, but practically it can take only discrete values. When we use an enormous number of sequences, we cannot avoid the effect of such errors.
%
Thus, the purpose of the experiments is only to confirm that the proposed value does not cause any problem in practical situation.
\par
Tables \ref{tab:proportion_1} and \ref{tab:proportion_2} show the results of proportion test with $\xi=3$ and with $\xi=2$, respectively. Tables \ref{tab:uniformity_1}, \ref{tab:uniformity_2} and \ref{tab:uniformity_3} show the results of uniformity test with the significance levels $0.0001$, $0.01$ and $0.05$, respectively. Note that ``MT/AES'' means that a tested sequence is generated by Mersenne Twister and AES-128 CTR is used for flipping each bit. ``AES/MT'' and ``AES/AES'' are defined in the same manner.
The results in Tables \ref{tab:proportion_1}, \ref{tab:proportion_2}, \ref{tab:uniformity_1}, \ref{tab:uniformity_2} and \ref{tab:uniformity_3} support that the proposed value is robust.
%-----------------------------------------------------------------------------------------------%
\begin{table}[htb]
  \centering
  \caption{Number of sets rejected by proportion test with $\xi=3$}
  \begin{tabular}{ccc} \hline
              & Yamamoto \cite{yamamoto2016highly}  & Proposed \\ \hline 
    MT/AES    & 0         & 0        \\
    AES/MT    & 0         & 0        \\
    AES/AES   & 0         & 0        \\ \hline 
  \end{tabular}
  \label{tab:proportion_1}
\end{table}
%-----------------------------------------------------------------------------------------------%
\begin{table}[htb]
  \centering
  \caption{Number of sets rejected by proportion test with $\xi=2$}
  \begin{tabular}{ccc} \hline
              & Yamamoto \cite{yamamoto2016highly} & Proposed \\ \hline 
    MT/AES    & 2         & 3        \\
    AES/MT    & 4         & 4        \\
    AES/AES   & 6         & 6        \\ \hline 
  \end{tabular}
  \label{tab:proportion_2}
\end{table}
%-----------------------------------------------------------------------------------------------%
\begin{table}[t]
  \centering
  \caption{Number of sets rejected by uniformity test with the significance level $0.0001$}
  \begin{tabular}{ccc} \hline
              & Yamamoto \cite{yamamoto2016highly} & Proposed \\ \hline 
    MT/AES    & 0         & 0        \\
    AES/MT    & 0         & 0        \\
    AES/AES   & 0         & 0        \\ \hline 
  \end{tabular}
  \label{tab:uniformity_1}
\end{table}
%-----------------------------------------------------------------------------------------------%
\begin{table}[t]
  \centering
  \caption{Number of sets rejected by uniformity test with the significance level $0.01$}
  \begin{tabular}{ccc} \hline
              & Yamamoto \cite{yamamoto2016highly} & Proposed \\ \hline 
    MT/AES    & 0         & 0        \\
    AES/MT    & 0         & 1        \\
    AES/AES   & 0         & 0        \\ \hline 
  \end{tabular}
  \label{tab:uniformity_2}
\end{table}
%-----------------------------------------------------------------------------------------------%
\begin{table}[t]
  \centering
  \caption{Number of sets rejected by uniformity test with the significance level $0.05$}
  \begin{tabular}{ccc} \hline
              & Yamamoto \cite{yamamoto2016highly} & Proposed \\ \hline 
    MT/AES    & 2         & 2        \\
    AES/MT    & 3         & 6        \\
    AES/AES   & 4         & 5        \\ \hline 
  \end{tabular}
  \label{tab:uniformity_3}
\end{table}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\clearpage
\section{Conclusion}\label{sec:conclusion}
\input{conclusion}
%-- Acknowledgments -------------------------------------------------------------
\clearpage
\acknowledgment
\input{acknowledments}
%-- References ------------------------------------------------------------------
\clearpage
\addcontentsline{toc}{section}{\refname} % Add to the table of contents.
                                         % Delete if you use chapter option.
\bibliographystyle{ieeetr}
\bibliography{cite}
%-- Appendix ---------------------------------------------------------------------
%%% If you don't need appendices, delete the below.
\clearpage
\appendix
\input{appendixA.tex}
\clearpage
% \input{appendixB.tex}
\newpage
\section{Exploration of the covariance given in Eq. (\ref{eq:covariance_g_g})}\label{appendix:B}
In this appendix, we explore the covariance given in Eq. (\ref{eq:covariance_g_g}) to calculate the value more efficiently by computational experiments. Notice that the covariance is written as 
\begin{align}\label{eq:cov_saikei}
	\mathrm{Cov}[g(A_n),\, g(A_{n+k})] 
	= \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} g(i) g(j) \mathrm{Pr} \left[ A_n=i,\,A_{n+k}=j \right] - \{L\times H(\hat{q})\}^2,
\end{align}
where $g$ is given in Eq. (\ref{eq:function_g}), $\mathrm{Pr} \left[ A_n=i,\,A_{n+k}=j \right]$ is given in Eq. (\ref{eq:joint_distribution}), and $H$ is a binary entropy function.
%
Since the term $\{L\times H(\hat{q})\}^2$ is irrelevant to an infinite series, we write the first term of the right hand side (r.h.s.) of Eq. (\ref{eq:cov_saikei}) as
%
\begin{align}\label{eq:Sk}
	\overline{S}_{k} := \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} g(i) g(j) \mathrm{Pr} \left[ A_n=i,\,A_{n+k}=j \right].
\end{align}
%
Firstly, we show the following Lemma concerning an infinite series to calculate the value expressed Eq. (\ref{eq:Sk}). 
%
\begin{lemma}\label{lemma:1}
% abcdefghijklmnopqrstuvwxyz
For any $z \in (0,1)$, the following relation holds
\begin{align}\label{eq:infinite_series}
	\sum_{i=1}^{\infty} g(i) \times (1-z)^i = -\frac{1}{\ln 2} \times \frac{1-z}{z} \times \ln z,
\end{align}
where
\begin{align}
	g(m) = (\log_2 \mathrm{e}) \sum_{k=1}^{m-1} \frac{1}{k}.
\end{align}
\end{lemma}
%-----------------------------------------------------------------------------------------------%
\begin{proof}
Let $t=1-z$, and $\overline{S}$ be the left hand side of Eq. (\ref{eq:infinite_series}), that is,
\begin{align}\begin{split}\label{eq:S}
  \overline{S} &= \sum_{i=1}^{\infty} g(i) \times t^i \\
    % \Leftrightarrow S &= g(1)\times A +  \sum_{i=2}^{\infty} g(i) \times A^{i} \label{eq:S}.
    &= g(1)\times t +  \sum_{i=2}^{\infty} g(i) \times t^{i}.
\end{split}\end{align}
Multiplying the both sides of Eq. (\ref{eq:S}) by $t(\neq 0)$, we obtain the following relation
\begin{align}\begin{split}\label{eq:AS}
  t\times \overline{S} &= \sum_{i=1}^{\infty} g(i) \times t^{i+1} \\
  &= \sum_{i=2}^{\infty} g(i-1) \times t^{i}.
\end{split}\end{align}
Subtracting Eq. (\ref{eq:AS}) from Eq. (\ref{eq:S}), we obtain the following relation
\begin{align}\begin{split}\label{eq:(1-A)S}
  (1-t)\overline{S} &= g(1)\times t^{1} + \sum_{i=2}^{\infty} \left\{ g(i)-g(i-1) \right\} \times t^{i} \\
  &=0 + \sum_{i=2}^{\infty} (\log_2 \mathrm{e}) \times \frac{1}{i-1} \times t^{i} \\
  &=(\log_2 \mathrm{e})\times t \times \sum_{i=1}^{\infty} \frac{t^{i}}{i} \\
  &=(\log_2 \mathrm{e})\times t \times \left\{ - \ln (1-t) \right\}.
\end{split}\end{align}
To obtain the last equality in the above equations, we have used the Taylor series for $|t| < 1$.
Dividing both sides of Eq. (\ref{eq:(1-A)S}) by $1-t (\neq 0)$, we arrive at the following result
\begin{align}
  \overline{S} &= -\frac{1}{\ln 2} \times \frac{t}{1-t} \times \ln (1-t).
\end{align}
The lemma is obtained if we substitute $t$ into $1-z$.
\end{proof}
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\par
In the next place, we calculate the value given in Eq. (\ref{eq:Sk}) more in details.
\subsection{Case of $1 \leq j \leq k-1$}
For $1 \leq j \leq k-1$, Eq. (\ref{eq:Sk}) is written as
\begin{align}\begin{split}
  \overline{S}_{k} 
  &:= \sum_{i=1}^{\infty} \sum_{j=1}^{k-1} g(i) g(j) \mathrm{Pr} \left[ A_n=i,\,A_{n+k}=j \right] \\
  &= \sum_{i=1}^{\infty} \sum_{j=1}^{k-1} g(i) g(j) \left( \sum_{r=0}^{L} \binom{L}{r}w_r^2 (1-w_r)^{i-1} \right) \times \left( \sum_{r=0}^{L} \binom{L}{r}w_r^2 (1-w_r)^{j-1} \right) \\ 
  &= \sum_{r=0}^{L} \left\{\binom{L}{r} \frac{w_r^2}{1-w_r} \sum_{i=1}^{\infty} g(i)(1-w_r)^{i} \right\}
  \times \sum_{r=0}^{L} \left\{ \binom{L}{r} \frac{w_r^2}{1-w_r} \sum_{j=1}^{k-1} g(j)(1-w_r)^{j} \right\}.
\end{split}\end{align}
From Lemma \ref{lemma:1}, the infinite series of $i$ in the above equations is written as
\begin{align}
	\sum_{i=1}^{\infty} g(i) (1 - w_r)^{i} = - \frac{1}{\ln 2} \times \frac{1-w_r}{w_r}\times\ln w_r.
\end{align}
%
Therefore, we obtain the following relation
\begin{align}\begin{split}
  \overline{S}_{k}
  =& \sum_{r=0}^{L} \left\{ \dbinom{L}{r} \frac{w_r^2}{1-w_r}  \times \left(- \frac{1}{\ln 2} \times \frac{1-w_r}{w_r}\times\ln w_r \right) \right\} \\
  &\times \sum_{r=0}^{L} \left\{ \dbinom{L}{r} \frac{w_r^2}{1-w_r} \times \sum_{j=1}^{k-1} (1-w_r)^{j} \right\} \\
  =&-\frac{1}{\ln 2}\sum_{r=0}^{L} \left\{ \dbinom{L}{r} w_r \ln w_r \right\} \times \sum_{r=0}^{L} \left\{ \dbinom{L}{r} \frac{w_r^2}{1-w_r} \sum_{j=1}^{k-1} (1-w_r)^{j} \right\}.
\end{split}\end{align}
%
\subsection{Case of $j=k$}
In the case of $j=k$, Eq. (\ref{eq:Sk}) is written as
\begin{align}\begin{split}\label{eq:app_case2}
  \overline{S}_{k} 
  &= \sum_{i=1}^{\infty} g(i) \sum_{r=0}^{L} \binom{L}{r} w_r^3 (1-w_r)^{k+i-2} \sum_{j \in \{k\}} g(j)\\
  &= \sum_{r=0}^{L} \binom{L}{r} w_r^3 (1-w_r)^{k-2} \sum_{i=1}^{\infty} g(i) (1-w_r)^{i} \times g(k) \\
  &= g(k) \times \sum_{r=0}^{L} \binom{L}{r} w_r^3 (1-w_r)^{k-2} \left( -\frac{1}{\ln 2} \times \frac{1-w_{r}}{w_{r}} \times \ln w_{r} \right) \\
  &= -\frac{g(k)}{\ln 2} \times \sum_{r=0}^{L} \binom{L}{r} w_r^2 (1-w_r)^{k-1} \ln w_{r}.
\end{split}\end{align}
The third equation in Eq. (\ref{eq:app_case2}) has been obtained from Lemma \ref{lemma:1}.
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Case of $k+1 \leq j \leq k+i-1$}
Recall that the joint distribution for $k+1 \leq j \leq k+i-1$ is written as
\begin{align}\begin{split}
	\mathrm{Pr}[A_n=i,\, A_{n+k}=j] 
	=& \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\mathrm{Pr}[e_3(b_1,b_2)] \\
	&+ \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1}\left\{\binom{L}{r_1}-1\right\}\mathrm{Pr}[e_3(b_1,b_2)],
\end{split}\end{align}
where $\mathrm{Pr}[e_3(b_1,b_2)]$ is expressed as
\begin{align}\begin{split}
	\mathrm{Pr}[e_3(b_1,b_2)]
  =& w_{r_1}^2  \times w_{r_2}^2 
  \times (1-w_{r_1})^{i-j+k-1} 
  \times (1-w_{r_1}-w_{r_2})^{j-k-1}
  \times (1-w_{r_2})^{k-1} \\
  =&\phi_k(r_1,r_2)\times (1-w_{r_1})^{i} \times \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j},
\end{split}\end{align}
%
where
\begin{align}\label{eq:phi_k}
	\phi_k(r_1,r_2) = w_{r_1}^2 w_{r_2}^2 
  	(1-w_{r_1})^{k-1} 
  	(1-w_{r_1}-w_{r_2})^{-k-1}
  	(1-w_{r_2})^{k-1}.
\end{align}
%
Then, Eq. (\ref{eq:Sk}) is expressed as
\begin{align}\begin{split}\label{eq:Sk_case3}
	\overline{S}_k 
	=& \sum_{i=1}^{\infty}\sum_{j=k+1}^{k+i-1} g(i)g(j) \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\mathrm{Pr}[e_3(b_1,b_2)]\\ 
	&+ \sum_{i=1}^{\infty}\sum_{j=k+1}^{k+i-1} g(i)g(j) \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1} \left\{\binom{L}{r_1}-1 \right\}\mathrm{Pr}[e_3(b_1,b_2)].
\end{split}\end{align}
%
Now, we consider the first term in r.h.s. of Eq. (\ref{eq:Sk_case3}). Let $\overline{A}_1$ be this term of Eq. (\ref{eq:Sk_case3}). We have
\begin{align}\begin{split}\label{eq:A_1}
	\overline{A}_1
	&=\sum_{i=1}^{\infty}\sum_{j=k+1}^{k+i-1} g(i)g(j) \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\mathrm{Pr}[e_3(b_1,b_2)] \\
	&=\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\phi_k(r_1,r_2)
	\sum_{i=1}^{\infty} \sum_{j=1}^{k+i-1} g(i)g(j)(1-w_{r_1})^{i} \times \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \\
	&=\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\phi_k(r_1,r_2)
	\sum_{j=k+1}^{\infty} \sum_{i=k+1}^{k+i-1} g(i)g(j)(1-w_{r_1})^{i} \times \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \\
	&=\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\phi_k(r_1,r_2)
	\sum_{j=k+1}^{\infty} \left\{ g(j) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \times \sum_{i=j-k+1}^{\infty} g(i)(1-w_{r_1})^{i} \right\}.
\end{split}\end{align}
In the course of the derivation of the above relations, the second equation has been obtained from Eq. (\ref{eq:phi_k}). The third equation has been obtained by exchanging the summation over $i$ and $j$.
%
Then, the infinite series with respect to $i$ in the last equation of Eq. (\ref{eq:A_1}) can be calculated as
\begin{align}\begin{split}
	\sum_{i=j-k+1}^{\infty} g(i)(1-w_{r_1})^{i} 
	&= \sum_{i=1}^{\infty} g(i)(1-w_{r_1})^{i} - \sum_{i=1}^{j-k} g(i)(1-w_{r_1})^{i} \\
	&= -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{i=1}^{j-k} g(i)(1-w_{r_1})^{i}.
\end{split}\end{align}
In the above relations, we have used the result of Lemma \ref{lemma:1}.
Hence, the first term in r.h.s. of Eq. (\ref{eq:Sk_case3}) can be expressed as
\begin{align}
	\overline{A}_1 
	=& \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\phi_k(r_1,r_2)\\
	&\times\sum_{j=k+1}^{\infty} \left[ g(j) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{i=1}^{j-k} g(i)(1-w_{r_1})^{i} \right\} \right].
\end{align}
We can derive the second term in r.h.s. of Eq. (\ref{eq:Sk_case3}) in the same way as the first term. Let $\overline{A}_2$ be the second term of Eq. (\ref{eq:Sk_case3}). Then, we have
\begin{align}\begin{split}
	\overline{A}_2 =& \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1}\left\{\binom{L}{r_1}-1\right\} \phi(r_1,r_2) \\
	&\times\sum_{j=k+1}^{\infty} \left[ g(j) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{i=1}^{j-k} g(i)(1-w_{r_1})^{i} \right\} \right].
\end{split}\end{align}
%
Therefore, Eq. (\ref{eq:Sk_case3}) can be written as
\begin{align}\begin{split}
	\overline{S}_k =& \overline{A}_1 + \overline{A}_2 \\
	=& \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\phi_k(r_1,r_2)\\
	&\times\sum_{j=k+1}^{\infty} \left[ g(j) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{i=1}^{j-k} g(i)(1-w_{r_1})^{i} \right\} \right] \\
	&+\sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1}\left\{\binom{L}{r_1}-1\right\} \phi(r_1,r_2) \\
	&\times\sum_{j=k+1}^{\infty} \left[ g(j) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{j} \times \left\{ -\frac{1}{\ln 2} 
	\times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{i=1}^{j-k} g(i)(1-w_{r_1})^{i} \right\} \right],
\end{split}\end{align}
where $\phi_k(r_1,r_2)$ is given in Eq. (\ref{eq:phi_k}).
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Case of $j=k+i$}
Equation (\ref{eq:Sk}) in the case of $j=k+i$ is equal to $0$ from Eq. (\ref{eq:joint_distribution}).
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
%-----------------------------------------------------------------------------------------------%
\subsection{Case of $j \geq k+i+1$}
%--------------------------------------------------------------------------------%
Recall that the joint distribution for $j \geq k+i+1$ is written as
\begin{align}\begin{split}
  \mathrm{Pr}[A_n=i,\, A_{n+k}=j] 
  =& \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\mathrm{Pr}[e_5(b_1,b_2)] \\
  &+ \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1}\left\{\binom{L}{r_1}-1\right\}\mathrm{Pr}[e_5(b_1,b_2)],
\end{split}\end{align}
where $\mathrm{Pr}[e_5(b_1,b_2)]$ is expressed as
\begin{align}\begin{split}
  \mathrm{Pr}[e_5(b_1,b_2)]
  =& w_{r_1}^2  \times w_{r_2}^2 
  \times (1-w_{r_1})^{-i+j-k-1} 
  \times (1-w_{r_1}-w_{r_2})^{i-1}
  \times (1-w_{r_1})^{k-1} \\
  =&\psi_k(r_1,r_2)\times \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times (1-w_{r_1})^j,
\end{split}\end{align}
%
where
\begin{align}\begin{split}
  \psi_k(r_1,r_2) 
  &= w_{r_1}^2 w_{r_2}^2 
    (1-w_{r_1})^{-k-1} 
    (1-w_{r_1}-w_{r_2})^{-1}
    (1-w_{r_1})^{k-1} \\
  &= w_{r_1}^2 w_{r_2}^2 (1-w_{r_1})^{-2} (1-w_{r_1}-w_{r_2})^{-1}.
  \label{eq:psi_k}
\end{split}\end{align}
%
Then, Eq. (\ref{eq:Sk}) is expressed as
\begin{align}\begin{split}\label{eq:Sk_case5}
  \overline{S}_k 
  =& \sum_{i=1}^{\infty}\sum_{j=k+i+1}^{\infty} g(i)g(j) \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\mathrm{Pr}[e_5(b_1,b_2)]\\ 
  &+ \sum_{i=1}^{\infty}\sum_{j=k+i+1}^{\infty} g(i)g(j) \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1} \left\{\binom{L}{r_1}-1 \right\}\mathrm{Pr}[e_5(b_1,b_2)].
\end{split}\end{align}
%
Firstly, we consider the first term in the r.h.s. of Eq. (\ref{eq:Sk_case3}). Let $\overline{B}_1$ be the first term of Eq. (\ref{eq:Sk_case3}). Then, $\overline{B}_1$ is written as
\begin{align}\begin{split}\label{eq:B_1}
  \overline{B}_1
  &=\sum_{i=1}^{\infty}\sum_{j=k+i+1}^{\infty} g(i)g(j) \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\mathrm{Pr}[e_5(b_1,b_2)] \\
  &=\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\psi_k(r_1,r_2)
  \sum_{i=1}^{\infty} \sum_{j=k+i+1}^{\infty} g(i)g(j) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times (1-w_{r_1})^j \\
  &=\sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\psi_k(r_1,r_2)
  \sum_{i=1}^{\infty} g(i)\left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times \sum_{j=k+i+1}^{\infty} g(j) (1-w_{r_1})^j.
\end{split}\end{align}
In the course of the derivation of the above relations, the second equation has been obtained from Eq. (\ref{eq:phi_k}). The third equation has been obtained by exchange of infinite series.
%
Then, the infinite sum with respect to $j$ in the last equation of Eq. (\ref{eq:B_1}) can be calculated as
\begin{align}\begin{split}
  \sum_{j=k+i+1}^{\infty} g(j) (1-w_{r_1})^j 
  &= \sum_{j=1}^{\infty} g(j)(1-w_{r_1})^{j} - \sum_{j=1}^{k+i} g(j)(1-w_{r_1})^{j} \\
  &= -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{j=1}^{k+i} g(j)(1-w_{r_1})^{j}.
\end{split}\end{align}
In the above relations, we use the result of Lemma \ref{lemma:1}.
Hence, the first term of Eq. (\ref{eq:Sk_case5}) can be expressed as
\begin{align}
  \overline{B}_1 
  =& \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\psi_k(r_1,r_2)\\
  &\times\sum_{i=1}^{\infty} \left[ g(i) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{j=1}^{k+i} g(j)(1-w_{r_1})^{j} \right\} \right].
\end{align}
We can derive the second term of Eq. (\ref{eq:Sk_case5}) in the same way as the first term. Let $\overline{B}_2$ be the second term of Eq. (\ref{eq:Sk_case5}). Then, we have
\begin{align}\begin{split}
  \overline{B}_2 =& \sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1}\left\{\binom{L}{r_1}-1\right\} \psi(r_1,r_2) \\
  &\times\sum_{i=1}^{\infty} \left[ g(i) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{j=1}^{k+i} g(j)(1-w_{r_1})^{j} \right\} \right].
\end{split}\end{align}
%
Therefore, Eq. (\ref{eq:Sk_case5}) can be expressed as
\begin{align}\begin{split}
  \overline{S}_k =& \overline{B}_1 + \overline{B}_2 \\
  =& \sum_{r_1=0}^{L} \sum_{r_2 \neq r_1} \binom{L}{r_1}\binom{L}{r_2}\psi_k(r_1,r_2)\\
  &\times\sum_{i=1}^{\infty} \left[ g(i) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{j=1}^{k+i} g(j)(1-w_{r_1})^{j} \right\} \right] \\
  &+\sum_{r_1=0}^{L} \sum_{r_2 \in \{r_1\}} \binom{L}{r_1}\left\{\binom{L}{r_1}-1\right\} \psi(r_1,r_2) \\
  &\times\sum_{i=1}^{\infty} \left[ g(i) \left(1-\frac{w_{r_2}}{1-w_{r_1}} \right)^{i} \times \left\{ -\frac{1}{\ln 2} \times \frac{1-w_{r_1}}{w_{r_1}} \times \ln w_{r_1} - \sum_{j=1}^{k+i} g(j)(1-w_{r_1})^{j} \right\} \right],
\end{split}\end{align}
where $\psi_k(r_1,r_2)$ is given in Eq. (\ref{eq:psi_k}).
%
%-- End of body -------------------------------------------------------------------
\fi
\ifoutputcover
\cleardoublepage
%-- Covers and abstract for submission --------------------------------------------
\makecover                      % Cover
\makespine[\numberofspines]     % Spine
\fi
\ifoutputabstractforsubmission
\makeabstractforsubmission      % Abstract for submission
\fi
\end{document}
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------